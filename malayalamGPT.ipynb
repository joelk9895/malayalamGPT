{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8b81b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "എന്റെ ഉപാസന __ Ente Upasana: നീലമരണം\n",
      "സുരേഷ് ഭായി തേങ്ങയടിക്കുന്നതു ആദ്യമായാണ്. :-)\n",
      "നേര്‍ത്ത അതിരിനാല് വേര്‍‌തിരിക്കപ്പെട്ടവര്. എന്നാല് മനസ്സുകൊണ്ടു വളരെ അകന്നവരും...\n",
      "ഭാവന വായനക്കാരനു വിട്ടിരിക്കുന്നു....\n",
      "മറ്റു പോസ്റ്റുകളില്‍ നിന്നും വ്യത്യസ്തമായൊരു അന്തരീക്ഷമാണല്ലോ ഇതില്‍.വായിച്ചു വന്നപ്പോള്‍ പേരും,ഏകാകിയായ താമസക്കാരനും ഒക്കെ കണ്ട് ബഷീറിന്റെ നീല വെളിച്ചം ഓര്‍മ്മ വന്നു.\n",
      "പക്ഷേ പിന്നീട് വായിച്ചു തീര്‍ന്നതറിഞ്ഞില്ല.ഒരു സ്വപ്നം കാണുന്ന പോലെ സുന്ദരമായി ഒഴുകിയൊഴുകി വായിച്ചു.:)\n",
      "ഗതകാലം ഒരു നൊമ്പരം, നീശന്‍... ഇപ്പോള്‍ ഇതാ ‘നീലമരണവും’!!\n",
      "പ്രിയ റോസ്,\n",
      "ഞാന്‍ താമസിക്കുന്നത് ബാംഗ്ലൂരില്‍ കെ‌ആര്‍ പുരത്തിനു അടുത്താണ്. ഇവിടെ അടുത്തു ജൂബിലി സ്കൂള്‍ എന്ന ഒരു സ്കൂള്‍ പ്രവര്‍ത്തിക്കുന്നുണ്ട്. ഒരു ആറുമാസം മുമ്പാണ് അവിടത്തെ ഒരു ടീച്ചര്‍ (അവിവാഹിതയായ യുവതി) അജ്ഞാതമായ കാരണങ്ങളാല്‍ ഷാളില്‍ തൂങ്ങിമരിച്ചത്. അതു എന്റെ മനസ്സില്‍ വെറുതെ കിടന്നു. പിന്നീട് പല കാരണങ്ങളാല്‍ ഞാന്‍ ഒരു പുതിയ റൂം അന്വേഷിക്കാന്‍ ഇടയായി, താമസത്തിനു. അങ്ങിനെ നോക്കിക്കണ്ട റൂമുകളില്‍ ഒന്നു ആ ടീച്ചറുടെ ബില്‍ഡിങ്ങിലെ ഒരു മുറിയായിരുന്നു (ആ മുറി അല്ല)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "with open('ml.txt', encoding='utf-8') as f:\n",
    "    f.seek(0, 2)\n",
    "    file_size = f.tell()\n",
    "    f.seek(0)\n",
    "    quarter_size = file_size // 80\n",
    "    text = f.read(quarter_size)\n",
    "\n",
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c261c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://data.statmt.org/cc-100/ml.txt.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23f7c268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'ഁ', 'ം', 'ഃ', 'അ', 'ആ', 'ഇ', 'ഈ', 'ഉ', 'ഊ', 'ഋ', 'ഌ', 'എ', 'ഏ', 'ഐ', 'ഒ', 'ഓ', 'ഔ', 'ക', 'ഖ', 'ഗ', 'ഘ', 'ങ', 'ച', 'ഛ', 'ജ', 'ഝ', 'ഞ', 'ട', 'ഠ', 'ഡ', 'ഢ', 'ണ', 'ത', 'ഥ', 'ദ', 'ധ', 'ന', 'ഩ', 'പ', 'ഫ', 'ബ', 'ഭ', 'മ', 'യ', 'ര', 'റ', 'ല', 'ള', 'ഴ', 'വ', 'ശ', 'ഷ', 'സ', 'ഹ', 'ഺ', '഼', 'ഽ', 'ാ', 'ി', 'ീ', 'ു', 'ൂ', 'ൃ', 'ൄ', 'െ', 'േ', 'ൈ', 'ൊ', 'ോ', 'ൌ', '്', 'ൎ', 'ൗ', 'ൟ', 'ൠ', 'ൡ', 'ൢ', '൦', '൧', '൨', '൩', '൪', '൫', '൬', '൭', '൮', '൯', '൰', '൱', '൲', '൹', 'ൺ', 'ൻ', 'ർ', 'ൽ', 'ൾ', 'ൿ']\n"
     ]
    }
   ],
   "source": [
    "filtered_text = re.sub(r'[^\\u0D00-\\u0D7F0-9 \\n]', '', text)\n",
    "\n",
    "print(sorted(set(filtered_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b17617a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "എന്റെ ഉപാസന    നീലമരണം\n",
      "സുരേഷ് ഭായി തേങ്ങയടിക്കുന്നതു ആദ്യമായാണ് \n",
      "നേര്ത്ത അതിരിനാല് വേര്തിരിക്കപ്പെട്ടവര് എന്നാല് മനസ്സുകൊണ്ടു വളരെ അകന്നവരും\n",
      "ഭാവന വായനക്കാരനു വിട്ടിരിക്കുന്നു\n",
      "മറ്റു പോസ്റ്റുകളില് നിന്നും വ്യത്യസ്തമായൊരു അന്തരീക്ഷമാണല്ലോ ഇതില്വായിച്ചു വന്നപ്പോള് പേരുംഏകാകിയായ താമസക്കാരനും ഒക്കെ കണ്ട് ബഷീറിന്റെ നീല വെളിച്ചം ഓര്മ്മ വന്നു\n",
      "പക്ഷേ പിന്നീട് വായിച്ചു തീര്ന്നതറിഞ്ഞില്ലഒരു സ്വപ്നം കാണുന്ന പോലെ സുന്ദരമായി ഒഴുകിയൊഴുകി വായിച്ചു\n",
      "ഗതകാലം ഒരു നൊമ്പരം നീശന് ഇപ്പോള് ഇതാ നീലമരണവും\n",
      "പ്രിയ റോസ്\n",
      "ഞാന് താമസിക്കുന്നത് ബാംഗ്ലൂരില് കെആര് പുരത്തിനു അടുത്താണ് ഇവിടെ അടുത്തു ജൂബിലി സ്കൂള് എന്ന ഒരു സ്കൂള് പ്രവര്ത്തിക്കുന്നുണ്ട് ഒരു ആറുമാസം മുമ്പാണ് അവിടത്തെ ഒരു ടീച്ചര് അവിവാഹിതയായ യുവതി അജ്ഞാതമായ കാരണങ്ങളാല് ഷാളില് തൂങ്ങിമരിച്ചത് അതു എന്റെ മനസ്സില് വെറുതെ കിടന്നു പിന്നീട് പല കാരണങ്ങളാല് ഞാന് ഒരു പുതിയ റൂം അന്വേഷിക്കാന് ഇടയായി താമസത്തിനു അങ്ങിനെ നോക്കിക്കണ്ട റൂമുകളില് ഒന്നു ആ ടീച്ചറുടെ ബില്ഡിങ്ങിലെ ഒരു മുറിയായിരുന്നു ആ മുറി അല്ല അതെന്നില് ചില ആശയങ്ങള് രൂപം കൊള്ളൊച്ചു അതിന്റെ പരിണതിയാണ് ഈ പോസ്റ്റ്\n",
      "ബഷീറിന്റ\n"
     ]
    }
   ],
   "source": [
    "print(filtered_text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1df6f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(set(filtered_text))\n",
    "\n",
    "# Encoder\n",
    "stoi = {char: idx for idx, char in enumerate(unique_chars)}\n",
    " \n",
    "# Decoder\n",
    "itos = {idx: char for idx, char in enumerate(unique_chars)}\n",
    "\n",
    "encode = lambda text: [stoi[char] for char in text if char in stoi]\n",
    "decode = lambda indices: ''.join([itos[idx] for idx in indices if idx in itos])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a521c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 48, 82, 57, 76, 1, 50, 77, 56, 82, 1, 36, 80, 55, 106, 1, 29, 76, 1, 36, 80, 105, 36, 82, 1, 16, 103, 1]\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"എന്റെ പേര് ജോയൽ കെ ജോർജ് ആൺ \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abe23d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23, 48, 82, 57, 76,  1, 19, 50, 69, 64, 48,  1,  1,  1,  1, 48, 71, 58,\n",
      "        54, 56, 43, 13,  0, 64, 72, 56, 77, 63, 82,  1, 53, 69, 55, 70,  1, 44,\n",
      "        77, 33, 82, 33, 55, 39, 70, 29, 82, 29, 72, 48, 82, 48, 44, 72,  1, 16,\n",
      "        46, 82, 55, 54, 69, 55, 69, 43, 82,  1,  0, 48, 77, 56, 82, 44, 82, 44,\n",
      "         1, 15, 44, 70, 56, 70, 48, 69, 58, 82,  1, 61, 77, 56, 82, 44, 70, 56,\n",
      "        70, 29, 82, 29, 50, 82, 50, 76, 39, 82, 39, 61, 56, 82,  1, 23, 48, 82,\n",
      "        48, 69, 58, 82,  1, 54, 48, 64, 82, 64, 72, 29, 79, 43, 82, 39, 72,  1,\n",
      "        61, 59, 56, 76,  1, 15, 29, 48, 82, 48, 61, 56, 72, 13,  0, 53, 69, 61,\n",
      "        48,  1, 61, 69, 55, 48, 29, 82, 29, 69, 56, 48, 72,  1, 61, 70, 39, 82,\n",
      "        39, 70, 56, 70, 29, 82, 29, 72, 48, 82, 48, 72,  0, 54, 57, 82, 57, 72,\n",
      "         1, 50, 80, 64, 82, 57, 82, 57, 72, 29, 59, 70, 58, 82,  1, 48, 70, 48,\n",
      "        82, 48, 72, 13,  1, 61, 82, 55, 44, 82, 55, 64, 82, 44, 54, 69, 55, 79,\n",
      "        56, 72,  1, 15, 48, 82, 44, 56, 71, 29, 82, 63, 54, 69, 43, 58, 82, 58,\n",
      "        80,  1, 17, 44, 70, 58, 82, 61, 69, 55, 70, 34, 82, 34, 72,  1, 61, 48,\n",
      "        82, 48, 50, 82, 50, 80, 59, 82,  1, 50, 77, 56, 72, 13, 24, 29, 69, 29,\n",
      "        70, 55, 69, 55,  1, 44, 69, 54, 64, 29, 82, 29, 69, 56, 48, 72, 13,  1,\n",
      "        26, 29, 82, 29, 76,  1, 29, 43, 82, 39, 82,  1, 52, 63, 71, 57, 70, 48,\n",
      "        82, 57, 76,  1, 48, 71, 58,  1, 61, 76, 59, 70, 34, 82, 34, 13,  1, 27,\n",
      "        56, 82, 54, 82, 54,  1, 61, 48, 82, 48, 72,  0, 50, 29, 82, 63, 77,  1,\n",
      "        50, 70, 48, 82, 48, 71, 39, 82,  1, 61, 69, 55, 70, 34, 82, 34, 72,  1,\n",
      "        44, 71, 56, 82, 48, 82, 48, 44, 57, 70, 38, 82, 38, 70, 58, 82, 58, 26,\n",
      "        56, 72,  1, 64, 82, 61, 50, 82, 48, 13,  1, 29, 69, 43, 72, 48, 82, 48,\n",
      "         1, 50, 80, 58, 76,  1, 64, 72, 48, 82, 46, 56, 54, 69, 55, 70,  1, 26,\n",
      "        60, 72, 29, 70, 55, 79, 60, 72, 29, 70,  1, 61, 69, 55, 70, 34, 82, 34,\n",
      "        72,  0, 31, 44, 29, 69, 58, 13,  1, 26, 56, 72,  1, 48, 79, 54, 82, 50,\n",
      "        56, 13,  1, 48, 71, 62, 48, 82,  1, 17, 50, 82, 50, 80, 59, 82,  1, 17,\n",
      "        44, 69,  1, 48, 71, 58, 54, 56, 43, 61, 72, 13,  0, 50, 82, 56, 70, 55,\n",
      "         1, 57, 80, 64, 82,  0, 38, 69, 48, 82,  1, 44, 69, 54, 64, 70, 29, 82,\n",
      "        29, 72, 48, 82, 48, 44, 82,  1, 52, 69, 13, 31, 82, 58, 73, 56, 70, 58,\n",
      "        82,  1, 29, 76, 16, 56, 82,  1, 50, 72, 56, 44, 82, 44, 70, 48, 72,  1,\n",
      "        15, 39, 72, 44, 82, 44, 69, 43, 82,  1, 17, 61, 70, 39, 76,  1, 15, 39,\n",
      "        72, 44, 82, 44, 72,  1, 36, 73, 52, 70, 58, 70,  1, 64, 82, 29, 73, 59,\n",
      "        82,  1, 23, 48, 82, 48,  1, 26, 56, 72,  1, 64, 82, 29, 73, 59, 82,  1,\n",
      "        50, 82, 56, 61, 56, 82, 44, 82, 44, 70, 29, 82, 29, 72, 48, 82, 48, 72,\n",
      "        43, 82, 39, 82,  1, 26, 56, 72,  1, 16, 57, 72, 54, 69, 64, 13,  1, 54,\n",
      "        72, 54, 82, 50, 69, 43, 82,  1, 15, 61, 70, 39, 44, 82, 44, 76,  1, 26,\n",
      "        56, 72,  1, 39, 71, 34, 82, 34, 56, 82,  1, 15, 61, 70, 61, 69, 65, 70,\n",
      "        44, 55, 69, 55,  1, 55, 72, 61, 44, 70,  1, 15, 36, 82, 38, 69, 44, 54,\n",
      "        69, 55,  1, 29, 69, 56, 43, 33, 82, 33, 59, 69, 58, 82,  1, 63, 69, 59,\n",
      "        70, 58, 82,  1, 44, 73, 33, 82, 33, 70, 54, 56, 70, 34, 82, 34, 44, 82,\n",
      "         1, 15, 44, 72,  1, 23, 48, 82, 57, 76,  1, 54, 48, 64, 82, 64, 70, 58,\n",
      "        82,  1, 61, 76, 57, 72, 44, 76,  1, 29, 70, 39, 48, 82, 48, 72,  1, 50,\n",
      "        70, 48, 82, 48, 71, 39, 82,  1, 50, 58,  1, 29, 69, 56, 43, 33, 82, 33,\n",
      "        59, 69, 58, 82,  1, 38, 69, 48, 82,  1, 26, 56, 72,  1, 50, 72, 44, 70,\n",
      "        55,  1, 57, 73, 13,  1, 15, 48, 82, 61, 77, 63, 70, 29, 82, 29, 69, 48,\n",
      "        82,  1, 17, 39, 55, 69, 55, 70,  1, 44, 69, 54, 64, 44, 82, 44, 70, 48,\n",
      "        72,  1, 15, 33, 82, 33, 70, 48, 76,  1, 48, 80, 29, 82, 29, 70, 29, 82,\n",
      "        29, 43, 82, 39,  1, 57, 73, 54, 72, 29, 59, 70, 58, 82,  1, 26, 48, 82,\n",
      "        48, 72,  1, 16,  1, 39, 71, 34, 82, 34, 57, 72, 39, 76,  1, 52, 70, 58,\n",
      "        82, 41, 70, 33, 82, 33, 70, 58, 76,  1, 26, 56, 72,  1, 54, 72, 57, 70,\n",
      "        55, 69, 55, 70, 56, 72, 48, 82, 48, 72,  1, 16,  1, 54, 72, 57, 70,  1,\n",
      "        15, 58, 82, 58,  1, 15, 44, 76, 48, 82, 48, 70, 58, 82,  1, 34, 70, 58,\n",
      "         1, 16, 62, 55, 33, 82, 33, 59, 82,  1, 56, 73, 50, 13,  1, 29, 79, 59,\n",
      "        82, 59, 79, 34, 82, 34, 72,  1, 15, 44, 70, 48, 82, 57, 76,  1, 50, 56,\n",
      "        70, 43, 44, 70, 55, 69, 43, 82,  1, 18,  1, 50, 80, 64, 82, 57, 82, 57,\n",
      "        82,  0, 52, 63, 71, 57, 70, 48, 82, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and vocabulary\n",
    "chars = sorted(set(train_data))  # Get unique characters from the training data\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # Character-to-index mapping\n",
    "itos = {i: ch for ch, i in stoi.items()}  # Index-to-character mapping\n",
    "vocab_size = len(stoi)  # Total number of unique characters in the vocabulary\n",
    "\n",
    "seq_len = 128  # Sequence length\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = TextDataset(train_data, stoi, seq_len)\n",
    "val_dataset = TextDataset(val_data, stoi, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbde84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Custom Multi-Head Attention module\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dim must be divisible by number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # Batch, Time, Channels\n",
    "        q = self.query(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n",
    "        k = self.key(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # Scaled dot-product attention\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = (attn_weights @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out(attn_output)\n",
    "\n",
    "# Feedforward network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.gelu(self.linear1(x)))\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = FeedForward(embed_dim, ff_hidden_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out = self.attention(x)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.layernorm1(x)\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        x = self.layernorm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# GPT Model\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, embed_dim, num_heads, num_layers, ff_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_hidden_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        position_ids = torch.arange(0, T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.token_embedding(x) + self.position_embedding(position_ids)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40f88827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, seq_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx+self.seq_len+1]\n",
    "        input_ids = torch.tensor([self.tokenizer[c] for c in chunk[:-1]], dtype=torch.long)\n",
    "        target_ids = torch.tensor([self.tokenizer[c] for c in chunk[1:]], dtype=torch.long)\n",
    "        return input_ids, target_ids\n",
    "    \n",
    "    \n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746dcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "ff_hidden_dim = 1024\n",
    "dropout = 0.1\n",
    "seq_len = 128\n",
    "\n",
    "model = GPT(vocab_size, seq_len, embed_dim, num_heads, num_layers, ff_hidden_dim, dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb589519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
